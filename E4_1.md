> 试证明对于不含冲突数据（即特征向量完全相同但标记不同）的训练集，必存在与训练集一致（即训练误差为 $0$）的决策树。

记训练集 $D=\{(\boldsymbol x_1,y_1),(\boldsymbol x_2,y_2),\cdots,(\boldsymbol x_m,y_m)\}$，属性集 $A=\{a_1,a_2,\cdots,a_d\}$，样本 $\boldsymbol x_i$ 各个属性的取值依次是 $x_{i1},x_{i2},\cdots,x_{id}$。考虑下列生成多变量决策树的过程：

1. $i\gets1$，生成根结点，作为当前结点。
2. 若 $y_i,y_{i+1},\cdots,y_m$都相同，将当前结点标记为 $y_i$，返回根结点。
3. 以 $a_1=x_{i1},a_2=x_{i2},\cdots,a_d=x_{id}$ 为划分条件，分支“是”为标记为 $y_i$ 的叶结点，分支“否”为新的当前结点。 
4. 若 $i< m$，则 $i\gets i+1$，回到步骤2；若 $i=m$，返回根结点。

容易验证，对于不含冲突数据的训练集，这样生成的决策树的训练误差为 $0$。

这样的决策树虽然训练误差为 $0$，但绝对是过拟合的，泛化误差可能很大。
